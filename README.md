# Rag-Pipeline-using-Ollama
This is simple Rag system pipeline in which we are providing bunch of urls manually or it can use google search for it for automatic update the database. Using the ollama opensource models llama3.2:3b and nomic-embed-text:latest for embeddings.

This project lets you **chat with Ollama LLMs** while using **web search
and retrieval** to provide factual, source-cited answers.\
It combines: - ğŸ” **Google Search + Filtering** â†’ Generate optimized
queries and pick relevant links\
- ğŸŒ **Crawling** â†’ Extract web content (using
[crawl4ai](https://pypi.org/project/crawl4ai/))\
- ğŸ“š **Vector DB (Chroma)** â†’ Store and retrieve knowledge chunks\
- ğŸ§  **Ollama LLM** â†’ Generate natural responses with context\
- ğŸ–¥ï¸ **Streamlit UI** â†’ Interactive chat with source citations

------------------------------------------------------------------------

## âœ¨ Features

-   âœ… **Toggle Web Search**: Let the model fetch fresh data from Google
    automatically\
-   âœ… **Manual URL Add**: Paste URLs to index specific pages\
-   âœ… **Retrieval-Augmented Generation (RAG)**: Use vector database to
    ground answers\
-   âœ… **Source Citations**: Every answer cites its origin in a neat
    sidebar\
-   âœ… **Knowledge Base Management**: Clear chat or wipe the knowledge
    base anytime

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    .
    â”œâ”€â”€ app.py              # Streamlit UI (chat + controls + citation view)
    â”œâ”€â”€ webUrlModule.py     # Google query generation + search + URL filtering
    â”œâ”€â”€ webCrawlModule.py   # Web crawling, RAG pipeline, vector DB, streaming responses
    â””â”€â”€ requirements.txt    # Python dependencies

------------------------------------------------------------------------

## âš™ï¸ Installation

### 1. Clone the repo

``` bash
git clone https://github.com/yourusername/webrag-ollama.git
cd webrag-ollama
```

### 2. Create a virtual environment

``` bash
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)
```

### 3. Install dependencies

``` bash
pip install -r requirements.txt
```

### 4. Install & run Ollama (for local LLMs)

-   [Install Ollama](https://ollama.ai/download)
-   Pull a model (example: LLaMA 3 3B)

``` bash
ollama pull llama3.2:3b
```

------------------------------------------------------------------------

## ğŸš€ Usage

### Start the app

``` bash
streamlit run app.py
```

### Sidebar Controls

-   **Use Retrieval (RAG)** â†’ Toggle knowledge base retrieval on/off\
-   **ğŸŒ Use Web Search** â†’ Auto-search Google, crawl results, and add
    to KB\
-   **Add Web Sources** â†’ Paste specific URLs to crawl & index\
-   **ğŸ§¹ Clear Chat** â†’ Reset conversation\
-   **ğŸ—‘ï¸ Clear KB** â†’ Wipe vector database

### Chat Window

-   Left column â†’ Chat with the assistant\
-   Right column â†’ **Sources (clickable URLs)** used in the answer

------------------------------------------------------------------------

## ğŸ› ï¸ Tech Stack

-   [Streamlit](https://streamlit.io/)
-   [Ollama](https://ollama.ai/)
-   [ChromaDB](https://www.trychroma.com/)
-   [LangChain](https://www.langchain.com/)
-   [crawl4ai](https://pypi.org/project/crawl4ai/)
